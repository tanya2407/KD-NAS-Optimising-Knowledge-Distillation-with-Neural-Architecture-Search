# KD-NAS-Optimising-Knowledge-Distillation-with-Neural-Architecture-Search
Develop and refine a Student-Teacher model using Knowledge Distillation(KD) tailored for a specific GLUE benchmark task, using a Language Model serving as the teacher model. This will be achieved by leveraging Neural Architecture Search(NAS) to systematically optimize the architecture of the student model.
